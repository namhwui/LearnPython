{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "useful_tricks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOdQNOtUHRVvW664KbvBUgc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namhwui/LearnPython/blob/main/useful_tricks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "Pgu58yHnavdY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAfRgVMKagRy"
      },
      "outputs": [],
      "source": [
        "import numpy as np # numpy; math operations\n",
        "import scipy as sp # scipy; has statistical functions and tests, including hierarchical clustering\n",
        "import pandas as pd # pandas\n",
        "import statsmodels as sm # statistical models with emphasis on parameter estimation and inference\n",
        "import sklearn.model_selection # has train_test_split, GridSearchCV\n",
        "import sklearn.linear_model # has regression models\n",
        "from sklearn.preprocessing import StandardScaler # scaling\n",
        "import auto_ts as at # automatic time series modelling; do 'pip install auto-ts' first \n",
        "import matplotlib.pyplot as plt # basic plotting library\n",
        "import seaborn as sns # fancy wrapper for vis\n",
        "import plotnine as p9 # ggplot in python"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pandas, Numpy Tricks"
      ],
      "metadata": {
        "id": "DWcHUGTPbllI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# seach through dict via key\n",
        "'key' in dct\n",
        "\n",
        "# index_col = [0] in pd.read_csv sets 1st column as primary key\n",
        "\n",
        "# get value associated with key\n",
        "dct['key']\n",
        "\n",
        "# get values from dictionary\n",
        "dct.values()\n",
        "\n",
        "\n",
        "# drop columns\n",
        "data.drop('col', axis = 1, inplace = True)\n",
        "\n",
        "# drop rows\n",
        "data.dropna(axis=0)\n",
        "\n",
        "# fill na\n",
        "data.fillna(0)\n",
        "\n",
        "# calculate multiple aggregate functions\n",
        "df.groupby(['variable']).agg({\n",
        "    'agg1': lambda x: # some function\n",
        "    'agg2': 'count',\n",
        "    'agg3': 'sum',\n",
        "    'agg4':['mean', 'count']})\n",
        "\n",
        "# rename columns in one go\n",
        "# inplace = True mutates the data frame\n",
        "df.rename(columns = {'agg1': 'col1',\n",
        "                     'agg2': 'col2',\n",
        "                     'agg3': 'col3'}, inplace=True)\n",
        "\n",
        "# count the occurrence of each cell in grouped data frame\n",
        "df.groupby(['a', 'b']).size()\n",
        "\n",
        "# find rows with missing data\n",
        "df.isnull().any(axis = 1)\n",
        "\n",
        "# rename columns\n",
        "df.rename(columns={'oldname1':'newname1', 'oldname2':'newname2'}, inplace = True)\n",
        "\n",
        "# check data type of each column\n",
        "df.dtypes\n",
        "# change data type\n",
        "df.column = df.column.astype('category')\n",
        "\n",
        "\n",
        "# count the unique elements in a series and their occurrence\n",
        "df['a'].value_counts()\n",
        "np.unique('array',  return_counts = True)\n",
        "\n",
        "# turns a (obs x column) dimensional data frame into  (obs x column) x 2 dimensional data,\n",
        "# where first column is variable and second column is its value\n",
        "pd.melt(df, id_vars = 'primary_key_here', var_name = 'column', value_name = 'value')\n",
        "\n",
        "# simplify/recode labels\n",
        "df['label2'] = df['label'].map({'label1':'else1', 'label2':'else1', 'label3':'else2'})\n",
        "\n",
        "# slicing in data frame gives you rows\n",
        "df[0:3] # gives rows 1,2\n",
        "\n",
        "# flip boolean element-wise when locating inside data frame with ~\n",
        "df[~(df.column == 'foo')]\n",
        "\n",
        "# when combining multiple boolean arrays for locating, bracket each condition and bind with & or | (for 'or')\n",
        "\n",
        "# find minimum element in a list and its position\n",
        "min('array')\n",
        "np.argmin('array')\n",
        "\n",
        "# drop last two characters in list of strings\n",
        "# 11th, 10th, 3rd, 8th, 7th: drop the last two\n",
        "num_th = data.Processor_Gen.isin(['11th', '10th', '3rd', '8th', '7th'])\n",
        "data.Processor_Gen[num_th] = data.Processor_Gen[num_th].str[:-2]\n",
        "\n",
        "# capitalise all\n",
        "data.Model = data.Model.str.upper()\n",
        "\n",
        "# turn date string to datetime\n",
        "data.date = pd.to_datetime(data.date) # this alone gives seconds too\n",
        "data.date = data.date.dt.date # in days; change dt.date to dt.week, or dt.month\n",
        "# get recency from date\n",
        "import datetime\n",
        "recency = max(df.date) + datetime.timedelta(days=1)"
      ],
      "metadata": {
        "id": "5RlKmqy6bpdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualisation Tricks\n"
      ],
      "metadata": {
        "id": "xz7wz_HJkufU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matplotlib and Seaborn"
      ],
      "metadata": {
        "id": "X-vS6MkAkz-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot columns over a grid\n",
        "plt.figure(figsize=(10, 10), dpi=80)\n",
        "for i, column in enumerate(data.columns[:-1], 1): # all columns except last\n",
        "    plt.subplot(4,3,i) # 4 = number of rows, 3 = number of columns\n",
        "    sns.histplot(x=data[column], hue = data.another_column)\n",
        "plt.tight_layout()\n",
        "plt.show\n",
        "\n",
        "# another way to plot over grid\n",
        "fig, ax = plt.subplots(1, 4, figsize=(13,5))\n",
        "for ii in range(len(Demographics)):\n",
        "    sns.countplot(x = Demographics[ii], ax = ax[ii], data = data)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# yet another example of plotting over grid\n",
        "fig, ax = plt.subplots(1, 3, figsize=(13,5))\n",
        "sns.scatterplot(x='col1', y='col2', hue = 'col3', data = data, ax = ax[0], alpha = 0.25)\n",
        "sns.scatterplot(x='col1', y='col2', hue = 'col3', data = data.loc[data.col3 == 'Yes', :], ax = ax[1])\n",
        "sns.scatterplot(x='col1', y='col2', hue = 'col3', data = data.loc[data.col3 == 'No', :], ax = ax[2])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# facet grid in seaborn\n",
        "# sharex = False gives each plot its own scale\n",
        "g = sns.FacetGrid(data.melt(var_name = 'column'), col = 'column', col_wrap = 4, sharex = False)\n",
        "g.map(sns.histplot, 'value')\n",
        "\n",
        "\n",
        "# histogram\n",
        "sns.histplot(x=data[column], hue = data.another_column)\n",
        "# density plot\n",
        "sns.displot(x=data[column], kde = True)\n",
        "# scatterplot\n",
        "sns.scatterplot(x='column1', y='column2', hue='optional', data=data)\n",
        "# bar plot\n",
        "sns.countplot(x = Demographics[ii], ax = ax[ii], data = data)\n",
        "# box plot\n",
        "sns.boxplot(x=data.col1, y=data[column], hue = data.col1)\n",
        "# mosaic plot\n",
        "from statsmodels.graphics.mosaicplot import mosaic\n",
        "plt.figure(figsize=(30, 30), dpi=80)\n",
        "mosaic(data, [Demographics[ii] for ii in [0, 1]], title = Demographics[0] + ' vs ' + Demographics[1])\n",
        "# pair plot\n",
        "sns.pairplot(data[Numerical])\n",
        "# line graph on multiple numerical columns\n",
        "data['Numerical columns'].set_index('index column').plot()\n",
        "data['Numerical columns'].plot()\n",
        "\n",
        "\n",
        "# line plot by cluster via melted data\n",
        "df_nor_melt = pd.melt(df_normalized.reset_index(),\n",
        "                      id_vars=['ID', 'Cluster'],\n",
        "                      value_vars=['Recency','Frequency','MonetaryValue'],\n",
        "                      var_name='Attribute',\n",
        "                      value_name='Value')\n",
        "df_nor_melt.head()\n",
        "sns.lineplot('Attribute', 'Value', hue='Cluster', data=df_nor_melt)\n"
      ],
      "metadata": {
        "id": "25Xd_vZZkyRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotnine"
      ],
      "metadata": {
        "id": "zqD9nrxNk3Rj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rating vs price, faceted by processor brand\n",
        "(p9_base\n",
        " + p9.geom_point(p9.aes(x = 'Rating', y = 'Price'))\n",
        " + p9.labs(title = 'Rating vs Price',\n",
        "           x = 'Rating',\n",
        "           y = 'Price')\n",
        " + p9.facet_wrap('Processor_Brand'))\n",
        "\n",
        "# another way to facet\n",
        "# the formula notation dictates the direction in which facet goes\n",
        "(p9.ggplot(data, p9.aes(x = 'reorder(Brand, Rate_Cost, fun = np.median)', y = 'Rate_Cost'))\n",
        "  + p9.geom_boxplot()\n",
        "  + p9.facet_grid('Processor_Brand ~ .'))\n",
        "\n",
        "\n",
        "p9_base = p9.ggplot(data)\n",
        "# grouped boxplot (by Brand variable)\n",
        "# reordered by median rating\n",
        "(p9_base\n",
        " + p9.geom_boxplot(p9.aes(x = 'reorder(Brand, Rating, fun = np.median)', y = 'Rating'))\n",
        " + p9.labs(title = \"Rating by Brand\",\n",
        "           x = 'Brand',\n",
        "           y = 'Rating'))"
      ],
      "metadata": {
        "id": "4GEhY0D5k42S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scipy Tricks"
      ],
      "metadata": {
        "id": "mQDUst9ak5eI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# F test for equality of mean over multiple groups\n",
        "for column in data.columns[:-1]:\n",
        "    a = data.loc[data.col1 == 3, column]\n",
        "    b = data.loc[data.col2 == 4, column]\n",
        "    c = data.loc[data.col3 == 5, column]\n",
        "    d = data.loc[data.col4 == 6, column]\n",
        "    e = data.loc[data.col5 == 7, column]\n",
        "    f = data.loc[data.col6 == 8, column]\n",
        "    print('F-test result for ' + column)\n",
        "    print(sp.stats.f_oneway(a, b, c, d, e, f))"
      ],
      "metadata": {
        "id": "Y1HFuoLzk-ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sklearn Tricks"
      ],
      "metadata": {
        "id": "bjvsqUnkk-3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standardising data"
      ],
      "metadata": {
        "id": "rHN1YXTtr63z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scales to (column-wise) mean zero and standard deviation 1\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X)\n",
        "data_scaled = scaler.transform(X)\n"
      ],
      "metadata": {
        "id": "6Ab8C6swlYTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train-test Split"
      ],
      "metadata": {
        "id": "9FfwVUG6lOEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# partition the data to save testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_scaled, \n",
        "                                                    data.quality_binary, \n",
        "                                                    test_size = 0.2, \n",
        "                                                    random_state = 42)"
      ],
      "metadata": {
        "id": "UqtgjZx8lRpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Selection with GridSearchCV"
      ],
      "metadata": {
        "id": "S2fPs66wlC7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GridSearchCV searches over pre-set hyperparameter space for optimal one via CV\n",
        "# output is the model fitted with optimal hyperparameter; so it has basic methods like fit, predict, etc.\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# recipe:\n",
        "param_grid = {'param1': ..., 'param2': ...}\n",
        "model = model_object(random_state = 111)\n",
        "model_grid = GridSearchCV(model, param_grid = param_grid, cv = 5)\n",
        "model_grid.fit(X_train, y_train)\n",
        "model_grid.predict(X_test)\n",
        "model_grid.predict_proba(X_test) # for models that can output probabilities"
      ],
      "metadata": {
        "id": "vHFSVPQYlL_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Useful Models"
      ],
      "metadata": {
        "id": "ZDvV0I1olnqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "GB = GradientBoostingClassifier()\n",
        "GB_param_grid = {\n",
        "    'learning_rate': [0.1, 0.15, 0.2, 0.25, 0.3],\n",
        "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
        "    'max_depth': [2, 3, 4, 5]\n",
        "}\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logistic = LogisticRegression()\n",
        "logistic_param_grid = {\n",
        "    Cs = np.linspace(0.01, 1)\n",
        "}\n",
        "\n",
        "from sklearn import svm\n",
        "KSVM = svm.SVC()\n",
        "KSVM_param_grid = {\n",
        "    'kernel': 'kernels',\n",
        "    'param_for_kernel':'kernel_specific',\n",
        "    'etc':'etc'\n",
        "}\n",
        "\n",
        "from sklearn.linear_modle import LinearRegression\n",
        "# not much to tune\n",
        "linreg = LinearRegression()\n",
        "\n",
        "from sklearn.linear_model import ElasticNet\n",
        "EN_param_grid = {\n",
        "    'alpha': [0.1, 0.5, 1, 2],\n",
        "    'l1_ratio': np.linspace(0, 1, num = 11)\n",
        "}\n",
        "EN = ElasticNet(random_state = 42)\n",
        "\n",
        "\n",
        "# Boosted regression\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "GBreg_param_grid = {\n",
        "    'learning_rate': [0.1, 0.15, 0.2, 0.25, 0.3],\n",
        "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
        "    'max_depth': [2, 3, 4, 5]\n",
        "}\n",
        "GBreg = GradientBoostingRegressor(random_state = 42)\n",
        "\n",
        "# Robust regression with Huber loss\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "HR_param_grid = {\n",
        "    'epsilon': np.linspace(1.0, 2.0, num = 11),\n",
        "    'alpha': np.linspace(0, 0.01, num = 11)\n",
        "}\n",
        "HR = HuberRegressor()\n",
        "\n",
        "\n",
        "# Gaussian mixture\n",
        "from sklearn.mixture import GaussianMixture\n",
        "BIC = []\n",
        "\n",
        "for g in range(6):\n",
        "    model = GaussianMixture(n_components = g + 1, random_state = 42).fit(data_numeric)\n",
        "    BIC.append(model.bic(data_numeric))\n",
        "\n",
        "G = np.argmin(BIC) + 1\n",
        "GMM = GaussianMixture(n_components = G, random_state = 42)\n",
        "label = GMM.fit_predict(data_numeric)"
      ],
      "metadata": {
        "id": "_vQL2rp7lscp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Validation"
      ],
      "metadata": {
        "id": "D7Vm2U3altEq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regression"
      ],
      "metadata": {
        "id": "Grt7aZCLvBkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unadjusted R^2 = SS_reg / SS_total\n",
        "from sklearn.metrics import r2_score\n",
        "print(round(r2_score(y_test, EN_pred), 3))\n",
        "\n",
        "# RSSE (divide by len(y_test) for RMSE)\n",
        "print(round(np.power(sum((y_test - EN_pred)**2), 0.5), 3))\n",
        "\n"
      ],
      "metadata": {
        "id": "yHVz4urxl3Nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Binary Classification"
      ],
      "metadata": {
        "id": "fDduYjP5vY7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def AUROC(truth, prob, pos_label):\n",
        "    from sklearn.metrics import roc_curve\n",
        "    from sklearn.metrics import RocCurveDisplay\n",
        "    \n",
        "    fpr, tpr, _ = roc_curve(truth, prob[:, 0], pos_label = pos_label)\n",
        "    roc_display = RocCurveDisplay(fpr = fpr, tpr = tpr).plot()\n",
        "\n",
        "# bunch of rates\n",
        "# behave = custom metric based on behavioural economics; pain from loss is approx twice the joy from gain\n",
        "# thus, false positive should be twice as painful as true positive is gainful\n",
        "# confusion matrix:\n",
        "# [[TP, FN],\n",
        "#  [FP, TN]]\n",
        "\n",
        "# recall/sensitivity = true positive rate = TP/(TP + FN)\n",
        "# precision = TP/(TP + FP)\n",
        "# false positive rate = type 1 error = FP/(FP+TN)\n",
        "# power = 1 - type 2 error = recall\n",
        "# specificity = true negative rate = TN/(TN + FP)\n",
        "# f1 = harmonic mean of precision & sensitivity = precision*sensitivity/(precision + sensitivity)\n",
        "\n",
        "def binary_classify_metric(truth, pred, pos_label, nround = 10):\n",
        "    from sklearn.metrics import f1_score\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    \n",
        "    f1 = f1_score(truth, pred, pos_label = pos_label)\n",
        "    conf_mat = confusion_matrix(truth, pred)\n",
        "    TPR = round(conf_mat[0, 0] / sum(conf_mat[0, :]), nround)\n",
        "    FPR = round(conf_mat[1, 0] / sum(conf_mat[1, :]), nround)\n",
        "    TNR = round(conf_mat[1, 1] / sum(conf_mat[1, :]), nround)\n",
        "    FNR = round(conf_mat[0, 1] / sum(conf_mat[0, :]), nround)\n",
        "    behave = 0.66 * FPR + 0.33 * TPR\n",
        "    \n",
        "    \n",
        "    val = {'f1':f1, 'TPR':TPR, 'FPR':FPR, 'TNR':TNR, 'FNR':FNR, 'behave':behave}\n",
        "    return val"
      ],
      "metadata": {
        "id": "TLIDi9CDvbCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clustering"
      ],
      "metadata": {
        "id": "LPQKCqIUwPLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adjusted rand index\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "adjusted_rand_score(label1, label2)"
      ],
      "metadata": {
        "id": "fAV-vSWuwOty"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}